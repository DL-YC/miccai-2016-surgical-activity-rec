{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "import collections\n",
    "import itertools\n",
    "from glob import glob\n",
    "import cPickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "matplotlib.rc('savefig', dpi=100)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "        \n",
    "    def __init__(self):\n",
    "        \"\"\" Create a Params object.\n",
    "        \n",
    "        This is a convenience class for all parameters. We can either a) edit\n",
    "        these parameters here or b) declare environment variables with the same\n",
    "        names. Any existing environment variables take priority over the values\n",
    "        listed here (this is useful for batch runs).\n",
    "        \n",
    "        Note: We define a sweep as a batch of sequences *that all have the same\n",
    "        length.* Shorter sequences are wrapped in time to match the length of\n",
    "        the longest sequence in a batch. From a modeling perspective, this means\n",
    "        that short sequences contribute just as much as long sequences to\n",
    "        overall loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.parser = argparse.ArgumentParser()\n",
    "        \n",
    "        self._add_param('num_layers', 1)\n",
    "        self._add_param('hidden_layer_size', 1024)\n",
    "        self._add_param('init_scale', 0.1)\n",
    "        self._add_param('dropout_keep_prob', 0.5)\n",
    "        \n",
    "        self._add_param('initial_learning_rate', 1.0)\n",
    "        self._add_param('num_initial_sweeps', 300)\n",
    "        self._add_param('num_sweeps_per_decay', 50)\n",
    "        self._add_param('decay_factor', 0.5)\n",
    "        self._add_param('max_global_grad_norm', 1.0)\n",
    "        \n",
    "        self._add_param('batch_size', 5)\n",
    "        self._add_param('num_train_sweeps', 600)\n",
    "        self._add_param('num_sweeps_per_summary', 7)\n",
    "        self._add_param('num_sweeps_per_save', 7)\n",
    "        \n",
    "        self._add_param('data_dir', '~/Data/JIGSAWS/Suturing')\n",
    "        self._add_param('data_filename', 'standardized_data.pkl')\n",
    "        self._add_param('test_users', 'B')\n",
    "        self._add_param('model_type', 'BidirectionalLSTM')\n",
    "        self._add_param('gpu_ind', 0)\n",
    "        self._add_param('gpu_mem_fraction', 1.0)\n",
    "        \n",
    "        self.args = self.parser.parse_args(args=[])\n",
    "        \n",
    "    def _add_param(self, name, default):\n",
    "        \"\"\" Add a parameter.\n",
    "        \n",
    "        The parameter is added with the specified default, but this default\n",
    "        is overridden by a corresponding environment variable if it exists.\n",
    "        \"\"\"\n",
    "        \n",
    "        param_type = type(default)\n",
    "        self.parser.add_argument('--%s' % name, type=param_type, default=os.environ.get(name, default))\n",
    "\n",
    "    @property\n",
    "    def num_layers(self):\n",
    "        \"\"\" An integer. The number of hidden layers. \"\"\"\n",
    "        return self.args.num_layers\n",
    "    \n",
    "    @property\n",
    "    def hidden_layer_size(self):\n",
    "        \"\"\" An integer. The number of hidden units per layer. \"\"\"\n",
    "        return self.args.hidden_layer_size\n",
    "    \n",
    "    @property\n",
    "    def init_scale(self):\n",
    "        \"\"\" A float. All model weights will be initialized using a uniform\n",
    "        distribution over `[-init_scale, init_scale]`.\"\"\"\n",
    "        return self.args.init_scale\n",
    "    \n",
    "    @property\n",
    "    def dropout_keep_prob(self):\n",
    "        \"\"\" A float. The fraction of inputs to keep whenever dropout is\n",
    "        applied. Dropout is applied to inputs/outputs of each time step, but\n",
    "        never across time steps.\"\"\"\n",
    "        return self.args.dropout_keep_prob\n",
    "    \n",
    "    @property\n",
    "    def initial_learning_rate(self):\n",
    "        \"\"\" A float. \"\"\"\n",
    "        return self.args.initial_learning_rate\n",
    "    \n",
    "    @property\n",
    "    def num_initial_sweeps(self):\n",
    "        \"\"\" An integer. The number of initial sweeps during which the learning\n",
    "        rate will remain fixed. \"\"\"\n",
    "        return self.args.num_initial_sweeps\n",
    "    \n",
    "    @property\n",
    "    def num_sweeps_per_decay(self):\n",
    "        \"\"\" An integer. The number of sweeps per decay once we exceed the\n",
    "        number of initial sweeps. \"\"\"\n",
    "        return self.args.num_sweeps_per_decay\n",
    "    \n",
    "    @property\n",
    "    def decay_factor(self):\n",
    "        \"\"\" A float. The factor by which we'll decay after exceeding the number\n",
    "        of initial sweeps. \"\"\"\n",
    "        return self.args.decay_factor\n",
    "    \n",
    "    @property\n",
    "    def max_global_grad_norm(self):\n",
    "        \"\"\" A float. All gradients are effectively concatenated to obtain the\n",
    "        global norm. If this global norm exceeds `max_global_grad_norm`, then\n",
    "        all gradients are scaled so that the global norm becomes\n",
    "        `max_global_grad_norm`.\"\"\"\n",
    "        return self.args.max_global_grad_norm\n",
    "    \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        \"\"\" An integer. The number of sequences in a batch/sweep. \"\"\"\n",
    "        return self.args.batch_size\n",
    "    \n",
    "    @property\n",
    "    def num_train_sweeps(self):\n",
    "        \"\"\" An integer. \"\"\"\n",
    "        return self.args.num_train_sweeps\n",
    "    \n",
    "    @property\n",
    "    def num_sweeps_per_summary(self):\n",
    "        \"\"\" An integer. How often we'll compute summaries during training. \"\"\"\n",
    "        return self.args.num_sweeps_per_summary\n",
    "    \n",
    "    @property\n",
    "    def num_sweeps_per_save(self):\n",
    "        \"\"\" An integer. How often we'll save the model during training. \"\"\"\n",
    "        return self.args.num_sweeps_per_save\n",
    "    \n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        \"\"\" A string. The directory containing our data. \"\"\"\n",
    "        return os.path.expanduser(self.args.data_dir)\n",
    "    \n",
    "    @property\n",
    "    def data_filename(self):\n",
    "        \"\"\" A string. The filename corresponding to the standardized Pickle\n",
    "        file that contains the data. This file should reside in `data_dir`.\"\"\"\n",
    "        return self.args.data_filename\n",
    "    \n",
    "    @property\n",
    "    def test_users(self):\n",
    "        \"\"\" A list of strings. The users that make up the test set. \"\"\"\n",
    "        return self.args.test_users.split(' ')\n",
    "    \n",
    "    @property\n",
    "    def model_type(self):\n",
    "        \"\"\" A string. 'ForwardLSTM', 'ReverseLSTM', or 'BidirectionalLSTM'. \"\"\"\n",
    "        return self.args.model_type\n",
    "    \n",
    "    @property\n",
    "    def gpu_ind(self):\n",
    "        \"\"\" An integer or None. If None, use CPUs. Otherwise, `gpu_ind` indexes\n",
    "        the `CUDA_VISIBLE_DEVICES` environment variable (which is already set if\n",
    "        you have CUDA appropriately set up on your machine). For example, if\n",
    "        `CUDA_VISIBLE_DEVICES` is '1,2', then `gpu_ind=1` will use GPU 2.\"\"\"\n",
    "        return self.args.gpu_ind\n",
    "    \n",
    "    @property\n",
    "    def gpu_mem_fraction(self):\n",
    "        \"\"\" A float. The fraction of memory for TensorFlow to acquire from the\n",
    "        GPU specified by `gpu_ind`.\"\"\"\n",
    "        return self.args.gpu_mem_fraction\n",
    "    \n",
    "    @property\n",
    "    def log_dir(self):\n",
    "        \"\"\" A string. A log directory that's based on the parameters. For\n",
    "        example, if the data directory is '~/Data/JIGSAWS', and we have only\n",
    "        one test user 'B', then the log directory will look something like\n",
    "        '~/Data/JIGSAWS/logs/BidirectionalLSTM_.../B'.\n",
    "        \n",
    "        Warning: This log directory will be deleted and replaced if it already\n",
    "        exists, so it should be unique for parameters/users and should\n",
    "        absolutely not be something generic like '~'.\n",
    "        \"\"\"\n",
    "        \n",
    "        params_str = '_'.join([self.model_type,\n",
    "                               '%d' % self.num_layers,\n",
    "                               '%03d' % self.hidden_layer_size,\n",
    "                               '%02d' % self.batch_size,\n",
    "                               '%.2f' % self.dropout_keep_prob,\n",
    "                               '%.4f' % self.initial_learning_rate])\n",
    "        \n",
    "        test_users_str = '_'.join(self.test_users)\n",
    "        \n",
    "        return os.path.join(self.data_dir, 'logs', params_str, test_users_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, pkl_path):\n",
    "        \"\"\" Create a Dataset object from a standardized Pickle file.\n",
    "        \n",
    "        JIGSAWS and MISTIC contain similar underlying data, namely kinematics\n",
    "        as input and surgical activity as output. This class loads a\n",
    "        standardized Pickle file that can contain data for either dataset. See\n",
    "        the properties below for a description of what the file must contain.\n",
    "        \n",
    "        Args:\n",
    "            pkl_path: A string. A path to the standardized Pickle file.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(pkl_path, 'r') as f:\n",
    "            self.pkl_dict = cPickle.load(f)\n",
    "\n",
    "        assert all(seq.shape[1] - 1 == self.input_size for seq in self.all_data.values())\n",
    "        \n",
    "    def get_seqs_by_user(self, user):\n",
    "        \"\"\" Get a list of sequences corresponding to a user.\n",
    "        \n",
    "        Args:\n",
    "            user: A string.\n",
    "        \n",
    "        Returns:\n",
    "            A list of sequences corresponding to `user`.\n",
    "        \"\"\"\n",
    "        \n",
    "        trial_names = sorted(self.user_to_trial_names[user])\n",
    "        seqs = [self.all_data[trial_name] for trial_name in trial_names]\n",
    "        return seqs\n",
    "    \n",
    "    def get_splits(self, test_users):\n",
    "        \"\"\" Get all sequences, split into a training set and a testing set.\n",
    "        \n",
    "        Args:\n",
    "            test_users: A list of strings.\n",
    "        \n",
    "        Returns:\n",
    "            A tuple,\n",
    "            train_seqs: A list of train sequences.\n",
    "            test_seqs: A list of test sequences.\n",
    "        \"\"\"\n",
    "        \n",
    "        train_users = [user for user in self.all_users if user not in test_users]\n",
    "        train_seqs = list(itertools.chain(*[self.get_seqs_by_user(user) for user in train_users]))\n",
    "        test_seqs = list(itertools.chain(*[self.get_seqs_by_user(user) for user in test_users]))\n",
    "        \n",
    "        # Sanity check\n",
    "        def seqs_are_same(seq1, seq2):\n",
    "            return seq1.shape == seq2.shape and np.allclose(seq1, seq2, rtol=1e-3, atol=1e-3)\n",
    "        for test_seq in test_seqs:\n",
    "            assert any([seqs_are_same(test_seq, test_seq_) for test_seq_ in test_seqs])\n",
    "            assert not any([seqs_are_same(test_seq, train_seq) for train_seq in train_seqs])\n",
    "        \n",
    "        return train_seqs, test_seqs\n",
    "    \n",
    "    @property\n",
    "    def dataset_name(self):\n",
    "        \"\"\" A string: the dataset name. \"\"\"\n",
    "        return self.pkl_dict['dataset_name']\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        \"\"\" A list of strings: the class names. \"\"\"\n",
    "        return self.pkl_dict['classes']\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\" An integer: the number of classes. \"\"\"\n",
    "        return self.pkl_dict['num_classes']\n",
    "    \n",
    "    @property\n",
    "    def all_users(self):\n",
    "        \"\"\" A list of strings, each representing a user. \"\"\"\n",
    "        return self.pkl_dict['all_users']\n",
    "    \n",
    "    @property\n",
    "    def all_trial_names(self):\n",
    "        \"\"\" A list of strings: all trial names over all users. \"\"\"\n",
    "        return self.pkl_dict['all_trial_names']\n",
    "    \n",
    "    @property\n",
    "    def user_to_trial_names(self):\n",
    "        \"\"\" A dictionary mapping users to trial-name lists. \"\"\"\n",
    "        return self.pkl_dict['user_to_trial_names']\n",
    "    \n",
    "    @property\n",
    "    def all_data(self):\n",
    "        \"\"\" A dictionary mapping trial names to NumPy arrays. Each NumPy\n",
    "            array has shape `[duration, input_size+1]`, with the last\n",
    "            column being class labels. \"\"\"\n",
    "        return self.pkl_dict['all_data']\n",
    "    \n",
    "    @property\n",
    "    def col_names(self):\n",
    "        \"\"\" A list of strings: the column names for each data column. \"\"\"\n",
    "        return self.pkl_dict['col_names']\n",
    "    \n",
    "    @property\n",
    "    def input_size(self):\n",
    "        \"\"\" An integer: the number of inputs per time step. \"\"\"\n",
    "        return self.all_data.values()[0].shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_seq(seq):\n",
    "    \"\"\" Normalize a sequence by centering/scaling columns.\n",
    "\n",
    "    Args:\n",
    "        seq: A 2-D NumPy array with shape `[duration, size]`.\n",
    "    \n",
    "    Returns:\n",
    "        A 2-D NumPy array with the same shape, but with all columns\n",
    "        having mean 0 and standard deviation 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    mu = seq.mean(axis=0, keepdims=True)\n",
    "    sigma = seq.std(axis=0, keepdims=True)\n",
    "    normalized_seq = (seq - mu) / sigma\n",
    "    return normalized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_raw_seq(seq):\n",
    "    \"\"\" Prepare a raw sequence for training/testing.\n",
    "    \n",
    "    This function a) splits a raw sequence into input and label sequences; b)\n",
    "    prepares a reset sequence (for handling RNN state resets); and c)\n",
    "    normalizes each input sequence.\n",
    "    \n",
    "    Args:\n",
    "        seq: A 2-D NumPy array with shape `[duration, num_inputs + 1]`.\n",
    "            The last column stores labels.\n",
    "            \n",
    "    Returns:\n",
    "        A tuple,\n",
    "        input_seq: A 2-D float32 NumPy array with shape\n",
    "            `[duration, num_inputs]`. A normalized input sequence.\n",
    "        reset_seq: A 2-D bool NumPy array with shape `[duration, 1]`.\n",
    "        label_seq: A 2-D int32 NumPy array with shape `[duration, 1]`.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_seq = seq[:, :-1].astype(np.float)\n",
    "    input_seq = normalize_seq(input_seq).astype(np.float32)\n",
    "    duration = input_seq.shape[0]\n",
    "    reset_seq = np.eye(1, duration, dtype=np.bool).T\n",
    "    label_seq = seq[:, -1:].astype(np.int32)\n",
    "    return input_seq, reset_seq, label_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    \"\"\" Convert labels to one-hot encodings.\n",
    "    \n",
    "    Args:\n",
    "        labels: A NumPy array of nonnegative labels.\n",
    "    \n",
    "    Returns:\n",
    "        A NumPy array with shape `labels.shape + [num_classes]`. That is,\n",
    "        the same shape is retained, but one axis is added for the one-hot\n",
    "        encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    encoding_matrix = np.zeros([labels.size, num_classes])\n",
    "    encoding_matrix[range(labels.size), labels.flatten()] = 1\n",
    "    encodings = encoding_matrix.reshape(list(labels.shape) + [num_classes])\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_ind_generator(num_seqs, shuffle=True):\n",
    "    \"\"\" A sequence-index generator.\n",
    "    \n",
    "    Args:\n",
    "        num_seqs: An integer. The number of sequences we'll be indexing.\n",
    "        shuffle: A boolean. If true, randomly shuffle indices epoch by epoch.\n",
    "    \n",
    "    Yields:\n",
    "        An integer in `[0, num_seqs)`.    \n",
    "    \"\"\"\n",
    "    \n",
    "    seq_inds = range(num_seqs)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(seq_inds)\n",
    "        for seq_ind in seq_inds:\n",
    "            yield seq_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sweep_generator(seq_list_list, batch_size, shuffle=False, num_sweeps=None):\n",
    "    \"\"\" Generate sweeps.\n",
    "    \n",
    "    Let's define a sweep as a collection of `batch_size` sequences that\n",
    "    continue together through time until all sequences in the batch have been\n",
    "    exhausted. Short sequences grow by being wrapped in time.\n",
    "    \n",
    "    Simplified example: pretend sequences are 1-D arrays, and that we have\n",
    "    `seq_list = [[1, 0], [1, 0, 0]]`. Then\n",
    "    `sweep_generator([seq_list], 3, shuffle=False)` would yield\n",
    "    `[ [[1, 0, 1], [1, 0, 0], [1, 0, 1]] ]`.\n",
    "    \n",
    "    Args:\n",
    "        seq_list_list: A list of sequence lists. The sequences in\n",
    "            `seq_list_list[0]` should correspond to the sequences in\n",
    "            `seq_list_list[1]`, in `seq_list_list[2]`, etc. Their durations\n",
    "            should be the same, but data types can differ. All sequences\n",
    "            should be 2-D and have time running along axis 0.\n",
    "        batch_size: An integer. The number of sequences in a batch.\n",
    "        shuffle: A boolean. If true, shuffle sequences epoch by epoch as we\n",
    "            populate sweeps.\n",
    "        num_sweeps: An integer. The number of sweeps to visit before the\n",
    "            generator is exhaused. If None, generate sweeps forever.\n",
    "    \n",
    "    Yields:\n",
    "        A list with the same length as `seq_list_list`. This contains a sweep\n",
    "        for the 1st seq list, a sweep for the 2nd seq list, etc., each sweep\n",
    "        being a NumPy array with shape `[batch_size, duration, ?]`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_sweeps is None:\n",
    "        num_sweeps = np.inf\n",
    "    \n",
    "    seq_durations = [len(seq) for seq in seq_list_list[0]]\n",
    "    num_seqs = len(seq_list_list[0])\n",
    "    seq_ind_gen = seq_ind_generator(num_seqs, shuffle=shuffle)\n",
    "    \n",
    "    for seq_list in seq_list_list:\n",
    "        assert len(seq_list) == num_seqs\n",
    "        assert [len(seq) for seq in seq_list] == seq_durations\n",
    "    \n",
    "    num_sweeps_visited = 0\n",
    "    while num_sweeps_visited < num_sweeps:\n",
    "        \n",
    "        new_seq_ind = [seq_ind_gen.next() for _ in xrange(batch_size)]\n",
    "        new_seq_durations = [seq_durations[i] for i in new_seq_ind]\n",
    "        longest_duration = np.max(new_seq_durations)\n",
    "        pad = lambda seq: np.pad(seq, [[0, longest_duration-len(seq)], [0, 0]], mode='wrap')\n",
    "        \n",
    "        new_sweep_list = []\n",
    "        for seq_list in seq_list_list:\n",
    "            new_seq_list = [seq_list[i] for i in new_seq_ind]\n",
    "            new_sweep = np.asarray([pad(seq) for seq in new_seq_list])\n",
    "            new_sweep_list.append(new_sweep)\n",
    "        \n",
    "        yield new_sweep_list\n",
    "        num_sweeps_visited += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    \n",
    "    def __init__(self, inputs, resets, training, params):\n",
    "        \"\"\" Create a long short-term memory RNN.\n",
    "        \n",
    "        This maps RNN inputs to RNN outputs. Computing predictions from the RNN\n",
    "        outputs needs to happen elsewhere.\n",
    "        \n",
    "        Args:\n",
    "            inputs: A 3-D float32 Tensor with shape\n",
    "                `[batch_size, duration, input_size]`.\n",
    "            resets: A 3-D bool Tensor with shape\n",
    "                `[batch_size, duration, 1]`. These indicate when sequences\n",
    "                reset (to handle states appropriately with sequences that have\n",
    "                been wrapped to form a sweep).\n",
    "            training: A 0-D bool Tensor. When False, dropout won't be applied.\n",
    "            params: A Params object.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.resets = resets\n",
    "        self.training = training\n",
    "        self.input_size = inputs.get_shape().as_list()[2]\n",
    "        \n",
    "        self.num_layers = params.num_layers\n",
    "        self.hidden_layer_size = params.hidden_layer_size\n",
    "        self.init_scale = params.init_scale\n",
    "        self.dropout_keep_prob = params.dropout_keep_prob\n",
    "        \n",
    "        batch_states_shape = tf.pack([tf.shape(inputs)[0], 2*self.hidden_layer_size])\n",
    "        self._batch_start_states = tf.zeros(batch_states_shape, dtype=tf.float32)\n",
    "        keep_prob = tf.cond(training, lambda: tf.constant(self.dropout_keep_prob), lambda: tf.constant(1.0))\n",
    "        \n",
    "        initializer = tf.random_uniform_initializer(-self.init_scale, self.init_scale)\n",
    "        with tf.variable_scope('LSTM', initializer=initializer):\n",
    "            \n",
    "            # We need to swap the batch, time axes since tf.scan will split\n",
    "            # along dimension 0.\n",
    "            inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "            resets = tf.transpose(resets, [1, 0, 2])\n",
    "            \n",
    "            states_list = []\n",
    "            prev_layer_outputs = tf.nn.dropout(inputs, keep_prob)\n",
    "            for layer in xrange(self.num_layers):\n",
    "                \n",
    "                def fixed_size_lstm_block(c_prev_and_m_prev, x_and_r):\n",
    "                    block_input_size = self.input_size if layer == 0 else self.hidden_layer_size\n",
    "                    return self._lstm_block(c_prev_and_m_prev, x_and_r, block_input_size)\n",
    "                \n",
    "                x_and_r = tf.concat(2, [prev_layer_outputs, tf.cast(resets, tf.float32)])\n",
    "                with tf.variable_scope('layer%d' % layer):\n",
    "                    c_and_m = tf.scan(fixed_size_lstm_block, x_and_r, initializer=self._batch_start_states)\n",
    "                states_list.append(c_and_m)\n",
    "                prev_layer_outputs = tf.nn.dropout(c_and_m[:, :, self.hidden_layer_size:], keep_prob)\n",
    "                \n",
    "            _states = tf.concat(2, [tf.expand_dims(states, 2) for states in states_list])\n",
    "            \n",
    "            # Now put the batch, time axes back.\n",
    "            self._states = tf.transpose(_states, [1, 0, 2, 3])\n",
    "            self._outputs = tf.transpose(prev_layer_outputs, [1, 0, 2])\n",
    "        \n",
    "    def _lstm_block(self, c_prev_and_m_prev, x_and_r, block_input_size):\n",
    "        \"\"\" LSTM block.\n",
    "        \n",
    "        This implementation uses a forget gate and peephole connections. Also,\n",
    "        sequence resets `r` are used here to handle state resets internally\n",
    "        instead of externally.\n",
    "\n",
    "        Args:\n",
    "            c_prev_and_m_prev: A 2-D float32 Tensor with shape\n",
    "                `[batch_size, 2*hidden_layer_size]`.\n",
    "            x_and_r: A 2-D float32 Tensor with shape\n",
    "                `[batch_size, block_input_size+1]`. It's a concatenation of\n",
    "                the block's inputs with sequence-reset booleans (though with\n",
    "                type float32). In the case of multiple layers, the inputs are\n",
    "                the previous layer's outputs.\n",
    "            block_input_size: An integer.\n",
    "\n",
    "        Returns:\n",
    "            The updated state c_and_m, with the same shape as\n",
    "            `c_prev_and_m_prev`.\n",
    "        \"\"\"\n",
    "        \n",
    "        def xmul(tensor, weights_name):\n",
    "            W = tf.get_variable(weights_name, shape=[block_input_size, self.hidden_layer_size])\n",
    "            return tf.matmul(tensor, W)\n",
    "        \n",
    "        def mmul(tensor, weights_name):\n",
    "            W = tf.get_variable(weights_name, shape=[self.hidden_layer_size, self.hidden_layer_size])\n",
    "            return tf.matmul(tensor, W)\n",
    "        \n",
    "        def diagcmul(tensor, weights_name):\n",
    "            w = tf.get_variable(weights_name, shape=[self.hidden_layer_size])\n",
    "            return tensor*w\n",
    "\n",
    "        def bias(name):\n",
    "            b = tf.get_variable(name, shape=[self.hidden_layer_size], initializer=tf.constant_initializer(0.0))\n",
    "            return b\n",
    "        \n",
    "        x = x_and_r[:, :block_input_size]\n",
    "        r = tf.cast(x_and_r[:, block_input_size], tf.bool)\n",
    "        \n",
    "        # If r[i] is True, revert back to initial states. Otherwise, keep\n",
    "        # the states from the previous time step.\n",
    "        c_prev_and_m_prev = tf.select(r, self._batch_start_states, c_prev_and_m_prev)\n",
    "        c_prev, m_prev = tf.split(1, 2, c_prev_and_m_prev)\n",
    "        \n",
    "        x_tilde = tf.tanh( xmul(x, 'W_xx') + mmul(m_prev, 'W_xm') + bias('b_x') )\n",
    "        i = tf.sigmoid( xmul(x, 'W_ix') + mmul(m_prev, 'W_im') + diagcmul(c_prev, 'w_ic') + bias('b_i') )\n",
    "        f = tf.sigmoid( xmul(x, 'W_fx') + mmul(m_prev, 'W_fm') + diagcmul(c_prev, 'w_fc') + bias('b_f') )\n",
    "        c = x_tilde*i + c_prev*f\n",
    "        o = tf.sigmoid( xmul(x, 'W_ox') + mmul(m_prev, 'W_om') + diagcmul(c, 'w_oc') + bias('b_o') )\n",
    "        m = o*tf.tanh(c)\n",
    "        \n",
    "        c_and_m = tf.concat(1, [c, m])\n",
    "        return c_and_m\n",
    "    \n",
    "    @property\n",
    "    def states(self):\n",
    "        \"\"\" A 4-D float32 Tensor with shape\n",
    "        `[batch_size, duration, num_layers, hidden_layer_size]`. \"\"\"\n",
    "        return self._states\n",
    "    \n",
    "    @property\n",
    "    def outputs(self):\n",
    "        \"\"\" A 3-D float32 Tensor with shape\n",
    "        `[batch_size, duration, hidden_layer_size]`. \"\"\"\n",
    "        return self._outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMModel(object):\n",
    "    \n",
    "    def __init__(self, input_size, target_size, params):\n",
    "        \"\"\" A base class for LSTM models that includes predictions and loss.\n",
    "        \n",
    "        Args:\n",
    "            input_size: An integer. The number of inputs per time step.\n",
    "            target_size: An integer. The dimensionality of the one-hot\n",
    "                encoded targets.\n",
    "            params: A Params object.       \n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.target_size = target_size\n",
    "        self.init_scale = params.init_scale\n",
    "        self.params = params\n",
    "        \n",
    "        self._inputs = tf.placeholder(tf.float32, shape=[None, None, input_size], name='inputs')\n",
    "        self._resets = tf.placeholder(tf.bool, shape=[None, None, 1], name='resets')\n",
    "        self._targets = tf.placeholder(tf.float32, shape=[None, None, target_size], name='targets')\n",
    "        self._training = tf.placeholder(tf.bool, shape=[], name='training')\n",
    "        \n",
    "        outputs = self._compute_rnn_outputs()\n",
    "        output_size = self._compute_rnn_output_size()\n",
    "        \n",
    "        initializer = tf.random_uniform_initializer(-self.init_scale, self.init_scale)\n",
    "        with tf.variable_scope('logits', initializer=initializer):\n",
    "            W = tf.get_variable('W', shape=[output_size, self.target_size])\n",
    "            b = tf.get_variable('b', shape=[self.target_size])\n",
    "            outputs_matrix = tf.reshape(outputs, [-1, output_size])\n",
    "            logits = tf.nn.xw_plus_b(outputs_matrix, W, b)\n",
    "            batch_size, duration, _ = tf.unpack(tf.shape(self.inputs))\n",
    "            logits_shape = tf.pack([batch_size, duration, self.target_size])\n",
    "            self._logits = tf.reshape(logits, logits_shape, name='logits')\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            logits = tf.reshape(self.logits, [-1, self.target_size])\n",
    "            targets = tf.reshape(self.targets, [-1, self.target_size])\n",
    "            cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits, targets)\n",
    "            self._loss = tf.reduce_mean(cross_entropies, name='loss')\n",
    "            \n",
    "    def _compute_rnn_outputs(self):\n",
    "        \"\"\" Compute RNN outputs.\n",
    "        \n",
    "        Returns:\n",
    "            A 3-D float32 Tensor with shape `[batch_size, duration, output_size]`.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _compute_rnn_output_size(self):\n",
    "        \"\"\" Compute RNN output size.\n",
    "        \n",
    "        It's not always the same as `hidden_layer_size`: in the\n",
    "        BidirectionalLSTM case, it's `2*hidden_layer_size`.\n",
    "        \n",
    "        Returns:\n",
    "            An integer.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        \"\"\" A 3-D float32 Placeholder with shape `[batch_size, duration, input_size]`. \"\"\"\n",
    "        return self._inputs\n",
    "    \n",
    "    @property\n",
    "    def resets(self):\n",
    "        \"\"\" A 3-D bool Placeholder with shape `[batch_size, duration, 1]`. \"\"\"\n",
    "        return self._resets\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        \"\"\" A 3-D float32 Placeholder with shape `[batch_size, duration, target_size]`. \"\"\"\n",
    "        return self._targets\n",
    "    \n",
    "    @property\n",
    "    def training(self):\n",
    "        \"\"\" A 0-D bool Placeholder. \"\"\"\n",
    "        return self._training\n",
    "    \n",
    "    @property\n",
    "    def logits(self):\n",
    "        \"\"\" A 3-D float32 Tensor with shape `[batch_size, duration, target_size]`. \"\"\"\n",
    "        return self._logits\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        \"\"\" A 0-D float32 Tensor. \"\"\"\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ForwardLSTMModel(LSTMModel):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        \"\"\" Create a forward LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            See `LSTMModel`.\n",
    "        \"\"\"\n",
    "        super(ForwardLSTMModel, self).__init__(*args)\n",
    "    \n",
    "    def _compute_rnn_outputs(self):\n",
    "        self._fw_lstm = LSTM(self.inputs, self.resets, self.training, self.params)\n",
    "        return self._fw_lstm.outputs\n",
    "    \n",
    "    def _compute_rnn_output_size(self):\n",
    "        return self._fw_lstm.hidden_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReverseLSTMModel(LSTMModel):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        \"\"\" Create a reverse LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            See `LSTMModel`.\n",
    "        \"\"\"\n",
    "        super(ReverseLSTMModel, self).__init__(*args)\n",
    "    \n",
    "    def _compute_rnn_outputs(self):\n",
    "        reversed_inputs = tf.reverse(self.inputs, [False, True, False])\n",
    "        reversed_resets = tf.reverse(self.resets, [False, True, False])\n",
    "        self._rv_lstm = LSTM(reversed_inputs, reversed_resets, self.training, self.params)\n",
    "        outputs = tf.reverse(self._rv_lstm.outputs, [False, True, False])\n",
    "        return outputs\n",
    "    \n",
    "    def _compute_rnn_output_size(self):\n",
    "        return self._rv_lstm.hidden_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BidirectionalLSTMModel(LSTMModel):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        \"\"\" Create a bidirectional LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            See `LSTMModel`.\n",
    "        \"\"\"\n",
    "        super(BidirectionalLSTMModel, self).__init__(*args)\n",
    "    \n",
    "    def _compute_rnn_outputs(self):\n",
    "        \n",
    "        reversed_inputs = tf.reverse(self.inputs, [False, True, False])\n",
    "        reversed_resets = tf.reverse(self.resets, [False, True, False])\n",
    "        with tf.variable_scope('fw'):\n",
    "            self._fw_lstm = LSTM(self.inputs, self.resets, self.training, self.params)\n",
    "        with tf.variable_scope('rv'):\n",
    "            self._rv_lstm = LSTM(reversed_inputs, reversed_resets, self.training, self.params)\n",
    "        \n",
    "        fw_outputs = self._fw_lstm.outputs\n",
    "        rv_outputs = tf.reverse(self._rv_lstm.outputs, [False, True, False])\n",
    "        outputs = tf.concat(2, [fw_outputs, rv_outputs])\n",
    "        return outputs\n",
    "    \n",
    "    def _compute_rnn_output_size(self):\n",
    "        return self._fw_lstm.hidden_layer_size + self._rv_lstm.hidden_layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def piecewise_constant(x, boundaries, values):\n",
    "    \"\"\" Piecewise constant function.\n",
    "    \n",
    "    Arguments:\n",
    "        x: A 0-D Tensor.\n",
    "        boundaries: A 1-D NumPy array with strictly increasing entries.\n",
    "        values: A 1-D NumPy array that specifies the values for the intervals\n",
    "            defined by `boundaries`. (It should therefore have one more entry\n",
    "            than `boundaries`.)\n",
    "    \n",
    "    Returns: A 0-D Tensor. Its value is `values[0]` when `x <= boundaries[0]`,\n",
    "        `values[1]` when `x > boundaries[0]` and `x <= boundaries[1]`, ..., and\n",
    "        values[-1] when `x > boundaries[-1]`.\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_fn_pairs = {}\n",
    "    pred_fn_pairs[x <= boundaries[0]] = lambda: tf.constant(values[0])\n",
    "    pred_fn_pairs[x > boundaries[-1]] = lambda: tf.constant(values[-1])\n",
    "    for lower, upper, value in zip(boundaries[:-1], boundaries[1:], values[1:-1]):\n",
    "        # We need to bind value here; can do this with lambda value=value: ...\n",
    "        pred_fn_pairs[(x > lower) & (x <= upper)] = lambda value=value: tf.constant(value)\n",
    "    \n",
    "    return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]), exclusive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    \n",
    "    def __init__(self, loss, params):\n",
    "        \"\"\" Create an optimizer.\n",
    "        \n",
    "        If the global norm over all gradients is greater than\n",
    "        `max_global_grad_norm`, then scale all gradients so that the global\n",
    "        norm becomes `max_global_grad_norm`. Then update using vanilla SGD with\n",
    "        a staircase learning-rate schedule which a) maintains the initial\n",
    "        learning rate for `params.num_initial_sweeps` sweeps and then b) decays\n",
    "        the learning rate by a factor of `params.decay_factor` every\n",
    "        `params.num_sweeps_per_decay` sweeps.\n",
    "        \n",
    "        Args:\n",
    "            loss: A 0-D float32 Tensor.\n",
    "            params: A Params object.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.initial_learning_rate = params.initial_learning_rate\n",
    "        self.num_initial_sweeps = params.num_initial_sweeps\n",
    "        self.num_sweeps_per_decay = params.num_sweeps_per_decay\n",
    "        self.decay_factor = params.decay_factor\n",
    "        self.max_global_grad_norm = params.max_global_grad_norm\n",
    "        self.num_train_sweeps = params.num_train_sweeps\n",
    "        \n",
    "        self._trainables = tf.trainable_variables()\n",
    "        self._raw_grads = tf.gradients(loss, self.trainables)\n",
    "        self._scaled_grads, _ = tf.clip_by_global_norm(self.raw_grads,\n",
    "                                                       clip_norm=self.max_global_grad_norm)\n",
    "        \n",
    "        self._num_sweeps_visited = tf.Variable(0, trainable=False, name='num_sweeps_visited', dtype=tf.int32)\n",
    "        boundaries = np.arange(self.num_initial_sweeps, self.num_train_sweeps,\n",
    "                               self.num_sweeps_per_decay, dtype=np.int32)\n",
    "        values = [self.initial_learning_rate * self.decay_factor**i\n",
    "                  for i in xrange(len(boundaries)+1)]\n",
    "        self._learning_rate = piecewise_constant(self.num_sweeps_visited, boundaries, values)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        grad_var_pairs = zip(self.scaled_grads, self.trainables)\n",
    "        self._optimize_op = optimizer.apply_gradients(grad_var_pairs, global_step=self.num_sweeps_visited)\n",
    "        \n",
    "    @property\n",
    "    def num_sweeps_visited(self):\n",
    "        \"\"\" A 0-D int32 Tensor. \"\"\"\n",
    "        return self._num_sweeps_visited\n",
    "    \n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        \"\"\" A 0-D float32 Tensor. \"\"\"\n",
    "        return self._learning_rate\n",
    "    \n",
    "    @property\n",
    "    def trainables(self):\n",
    "        \"\"\" A list of trainable variables that will be updated. \"\"\"\n",
    "        return self._trainables\n",
    "    \n",
    "    @property\n",
    "    def raw_grads(self):\n",
    "        \"\"\" A list of raw gradient Tensors. \"\"\"\n",
    "        return self._raw_grads\n",
    "    \n",
    "    @property\n",
    "    def scaled_grads(self):\n",
    "        \"\"\" A list of scaled gradient Tensors. \"\"\"\n",
    "        return self._scaled_grads\n",
    "    \n",
    "    @property\n",
    "    def optimize_op(self):\n",
    "        \"\"\" An Operation that takes one optimization step. \"\"\"\n",
    "        return self._optimize_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(sess, model, input_seqs, reset_seqs):\n",
    "    \"\"\" Compute prediction sequences from input sequences.\n",
    "    \n",
    "    Args:\n",
    "        sess: A Session.\n",
    "        model: An LSTMModel.\n",
    "        input_seqs: A list of input sequences, each a float32 NumPy array with\n",
    "            shape `[duration, input_size]`.\n",
    "        reset_seqs: A list of reset sequences, each a bool NumPy array with\n",
    "            shape `[duration, 1]`.\n",
    "    \n",
    "    Returns:\n",
    "        A list of prediction sequences, each a NumPy array with shape\n",
    "        `[duration, 1]`, containing predicted labels for each time step.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(input_seqs)\n",
    "    seq_durations = [len(seq) for seq in input_seqs]\n",
    "    input_sweep, reset_sweep = sweep_generator([input_seqs, reset_seqs], batch_size=batch_size).next()\n",
    "    \n",
    "    logit_sweep = sess.run(model.logits, feed_dict={model.inputs: input_sweep,\n",
    "                                                    model.resets: reset_sweep,\n",
    "                                                    model.training: False})\n",
    "    \n",
    "    logit_seqs = [seq[:duration] for (seq, duration) in zip(logit_sweep, seq_durations)]\n",
    "    prediction_seqs = [np.argmax(seq, axis=1).reshape(-1, 1) for seq in logit_seqs]\n",
    "    \n",
    "    return prediction_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(prediction_seq, label_seq):\n",
    "    \"\"\" Compute accuracy, the fraction of correct predictions.\n",
    "    \n",
    "    Args:\n",
    "        prediction_seq: A 2-D int NumPy array with shape\n",
    "            `[duration, 1]`.\n",
    "        label_seq: A 2-D int NumPy array with the same shape.\n",
    "    \n",
    "    Returns:\n",
    "        A float.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.mean(prediction_seq == label_seq, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_edit_distance(prediction_seq, label_seq):\n",
    "    \"\"\" Compute segment-level edit distance.\n",
    "    \n",
    "    First, transform each sequence to the segment level by replacing any\n",
    "    repeated, adjacent labels with one label. Second, compute the edit distance\n",
    "    (Levenshtein distance) between the two segment-level sequences.\n",
    "    \n",
    "    Simplified example: pretend each input sequence is only 1-D, with\n",
    "    `prediction_seq = [1, 3, 2, 2, 3]` and `label_seq = [1, 2, 2, 2, 3]`.\n",
    "    The segment-level equivalents are `[1, 3, 2, 3]` and `[1, 2, 3]`, resulting\n",
    "    in an edit distance of 1.\n",
    "    \n",
    "    Args:\n",
    "        prediction_seq: A 2-D int NumPy array with shape\n",
    "            `[duration, 1]`.\n",
    "        label_seq: A 2-D int NumPy array with shape\n",
    "            `[duration, 1]`.\n",
    "    \n",
    "    Returns:\n",
    "        A nonnegative integer, the number of operations () to transform the\n",
    "        segment-level version of `prediction_seq` into the segment-level\n",
    "        version of `label_seq`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def edit_distance(seq1, seq2):\n",
    "\n",
    "        seq1 = [-1] + list(seq1)\n",
    "        seq2 = [-1] + list(seq2)\n",
    "\n",
    "        dist_matrix = np.zeros([len(seq1), len(seq2)], dtype=np.int)\n",
    "        dist_matrix[:, 0] = np.arange(len(seq1))\n",
    "        dist_matrix[0, :] = np.arange(len(seq2))\n",
    "\n",
    "        for i in xrange(1, len(seq1)):\n",
    "            for j in xrange(1, len(seq2)):\n",
    "                if seq1[i] == seq2[j]:\n",
    "                    dist_matrix[i, j] = dist_matrix[i-1, j-1]\n",
    "                else:\n",
    "                    operation_dists = [dist_matrix[i-1, j], dist_matrix[i, j-1], dist_matrix[i-1, j-1]]\n",
    "                    dist_matrix[i, j] = np.min(operation_dists) + 1\n",
    "\n",
    "        return dist_matrix[-1, -1]\n",
    "    \n",
    "    def segment_level(seq):\n",
    "        segment_level_seq = []\n",
    "        for label in seq.flatten():\n",
    "            if len(segment_level_seq) == 0 or segment_level_seq[-1] != label:\n",
    "                segment_level_seq.append(label)\n",
    "        return segment_level_seq\n",
    "    \n",
    "    return edit_distance(segment_level(prediction_seq), segment_level(label_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(prediction_seqs, label_seqs):\n",
    "    \"\"\" Compute metrics averaged over sequences.\n",
    "    \n",
    "    Args:\n",
    "        prediction_seqs: A list of int NumPy arrays, each with shape\n",
    "            `[duration, 1]`.\n",
    "        label_seqs: A list of int NumPy arrays, each with shape\n",
    "            `[duration, 1]`.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple,\n",
    "        mean_accuracy: A float, the average over all sequences of the\n",
    "            accuracies computed on a per-sequence basis.\n",
    "        mean_edit_distance: A float, the average over all sequences of\n",
    "            edit distances computed on a per-sequence basis.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = [compute_accuracy(pred_seq, label_seq)\n",
    "                  for (pred_seq, label_seq) in zip(prediction_seqs, label_seqs)]\n",
    "    edit_dists = [compute_edit_distance(pred_seq, label_seq)\n",
    "                  for (pred_seq, label_seq) in zip(prediction_seqs, label_seqs)]\n",
    "    return np.mean(accuracies, dtype=np.float), np.mean(edit_dists, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_seq(label_seq, num_classes, y_value):\n",
    "    \"\"\" Plot a label sequence.\n",
    "    \n",
    "    The sequence will be shown using a horizontal colored line, with colors\n",
    "    corresponding to classes.\n",
    "    \n",
    "    Args:\n",
    "        label_seq: An int NumPy array with shape `[duration, 1]`.\n",
    "        num_classes: An integer.\n",
    "        y_value: A float. The y value at which the horizontal line will sit.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_seq = label_seq.flatten()\n",
    "    x = np.arange(0, label_seq.size)\n",
    "    y = y_value*np.ones(label_seq.size)\n",
    "    plt.scatter(x, y, c=label_seq, marker='|', lw=2, vmin=0, vmax=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize(prediction_seqs, label_seqs, num_classes, fig_width=6.5, fig_height_per_seq=0.5):\n",
    "    \"\"\" Visualize predictions vs. ground truth.\n",
    "    \n",
    "    Args:\n",
    "        prediction_seqs: A list of int NumPy arrays, each with shape\n",
    "            `[duration, 1]`.\n",
    "        label_seqs: A list of int NumPy arrays, each with shape `[duration, 1]`.\n",
    "        num_classes: An integer.\n",
    "        fig_width: A float. Figure width (inches).\n",
    "        fig_height_per_seq: A float. Figure height per sequence (inches).\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of the created figure, axes.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_seqs = len(label_seqs)\n",
    "    max_seq_length = max([seq.shape[0] for seq in label_seqs])\n",
    "    figsize = (fig_width, num_seqs*fig_height_per_seq)\n",
    "    fig, axes = plt.subplots(nrows=num_seqs, ncols=1, sharex=True, figsize=figsize)\n",
    "    \n",
    "    for pred_seq, label_seq, ax in zip(prediction_seqs, label_seqs, axes):\n",
    "        plt.sca(ax)\n",
    "        plot_seq(label_seq, num_classes, 1)\n",
    "        plot_seq(pred_seq, num_classes, -1)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.xlim(0, max_seq_length)\n",
    "        plt.ylim(-2.75, 2.75)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, model, optimizer, train_input_seqs, train_reset_seqs, train_label_seqs,\n",
    "          test_input_seqs, test_reset_seqs, test_label_seqs, params):\n",
    "    \"\"\" Train a model.\n",
    "    \n",
    "    This function trains a model, and during training exports TensorBoard\n",
    "    summaries for both the train set and the test set.\n",
    "    \n",
    "    Args:\n",
    "        sess: A Session.\n",
    "        model: An LSTMModel.\n",
    "        optimizer: An Optimizer.\n",
    "        train_input_seqs: A list of 2-D float NumPy arrays, each with shape\n",
    "            `[duration, input_size]`.\n",
    "        train_reset_seqs: A list of 2-D bool NumPy arrays, each with shape\n",
    "            `[duration, 1]`.\n",
    "        train_label_seqs: A list of 2-D int NumPy arrays, each with shape\n",
    "            `[duration, 1]`.\n",
    "        test_input_seqs: A list of 2-D float NumPy arrays, each with shape\n",
    "            `[duration, input_size]`.\n",
    "        test_reset_seqs: A list of 2-D bool NumPy arrays, each with shape\n",
    "            `[duration, 1]`.\n",
    "        test_label_seqs: A list of 2-D int NumPy arrays, each with shape\n",
    "            `[duration, 1]`.\n",
    "        params: A Params object.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_dir = params.log_dir\n",
    "    batch_size = params.batch_size\n",
    "    num_train_sweeps = params.num_train_sweeps\n",
    "    num_sweeps_per_summary = params.num_sweeps_per_summary\n",
    "    num_sweeps_per_save = params.num_sweeps_per_save\n",
    "    \n",
    "    tf.scalar_summary('learning_rate', optimizer.learning_rate)\n",
    "\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "    update_train_loss_ema = ema.apply([model.loss])\n",
    "    train_loss_ema = ema.average(model.loss)\n",
    "    tf.scalar_summary('train_loss_ema', train_loss_ema)\n",
    "    \n",
    "    train_accuracy = tf.placeholder(tf.float32, name='train_accuracy')\n",
    "    train_edit_dist = tf.placeholder(tf.float32, name='train_edit_dist')\n",
    "    test_accuracy = tf.placeholder(tf.float32, name='test_accuracy')\n",
    "    test_edit_dist = tf.placeholder(tf.float32, name='test_edit_dist')\n",
    "    values = [train_accuracy, train_edit_dist, test_accuracy, test_edit_dist]\n",
    "    tags = [value.op.name for value in values]\n",
    "    tf.scalar_summary(tags, tf.pack(values))\n",
    "    \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "    summary_writer = tf.train.SummaryWriter(logdir=log_dir, graph=sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    num_sweeps_visited = 0\n",
    "    start_time = time.time()\n",
    "    train_gen = sweep_generator([train_input_seqs, train_reset_seqs, train_label_seqs],\n",
    "                                batch_size=batch_size, shuffle=True, num_sweeps=None)\n",
    "    while num_sweeps_visited <= num_train_sweeps:\n",
    "        \n",
    "        if num_sweeps_visited % num_sweeps_per_summary == 0:\n",
    "            \n",
    "            train_prediction_seqs = predict(sess, model, train_input_seqs, train_reset_seqs)\n",
    "            train_accuracy_, train_edit_dist_ = compute_metrics(train_prediction_seqs, train_label_seqs)\n",
    "            test_prediction_seqs = predict(sess, model, test_input_seqs, test_reset_seqs)\n",
    "            test_accuracy_, test_edit_dist_ = compute_metrics(test_prediction_seqs, test_label_seqs)\n",
    "            summary = sess.run(summary_op, feed_dict={train_accuracy: train_accuracy_,\n",
    "                                                      train_edit_dist: train_edit_dist_,\n",
    "                                                      test_accuracy: test_accuracy_,\n",
    "                                                      test_edit_dist: test_edit_dist_})\n",
    "            summary_writer.add_summary(summary, global_step=num_sweeps_visited)\n",
    "            \n",
    "            with open(os.path.join(log_dir, 'status.txt'), 'w') as f:\n",
    "                line = '%05.1f      ' % ((time.time() - start_time)/60)\n",
    "                line += '%04d      ' % num_sweeps_visited\n",
    "                line += '%.6f  %08.3f     ' % (train_accuracy_, train_edit_dist_)\n",
    "                line += '%.6f  %08.3f     ' % (test_accuracy_, test_edit_dist_)\n",
    "                print(line, file=f)\n",
    "            \n",
    "            with open(os.path.join(log_dir, 'test_label_seqs.pkl'), 'w') as f:\n",
    "                cPickle.dump(test_label_seqs, f)\n",
    "            with open(os.path.join(log_dir, 'test_prediction_seqs.pkl'), 'w') as f:\n",
    "                cPickle.dump(test_prediction_seqs, f)\n",
    "                \n",
    "            fig, axes = visualize(test_prediction_seqs, test_label_seqs, model.target_size)\n",
    "            axes[0].set_title(line)\n",
    "            display.display(fig)\n",
    "            display.clear_output(wait=True)\n",
    "                \n",
    "        if num_sweeps_visited % num_sweeps_per_save == 0:\n",
    "            saver.save(sess, os.path.join(log_dir, 'model.ckpt'))\n",
    "        \n",
    "        train_inputs, train_resets, train_labels = train_gen.next()\n",
    "        # We squeeze here because otherwise the targets would have shape\n",
    "        # [batch_size, duration, 1, num_classes].\n",
    "        train_targets = one_hot(train_labels, model.target_size).squeeze(axis=2)\n",
    "        _, _, num_sweeps_visited = sess.run(\n",
    "            [optimizer.optimize_op, update_train_loss_ema, optimizer.num_sweeps_visited],\n",
    "            feed_dict={model.inputs: train_inputs,\n",
    "                       model.resets: train_resets,\n",
    "                       model.targets: train_targets,\n",
    "                       model.training: True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training(params):\n",
    "    \"\"\" Run training for a single configuration.\n",
    "    \n",
    "    Args:\n",
    "        params: A Params object.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_dir = params.data_dir\n",
    "    data_filename = params.data_filename\n",
    "    test_users = params.test_users\n",
    "    model_type = params.model_type\n",
    "    gpu_ind = params.gpu_ind\n",
    "    gpu_mem_fraction = params.gpu_mem_fraction\n",
    "    log_dir = params.log_dir\n",
    "    \n",
    "    pkl_path = os.path.join(data_dir, data_filename)\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise ValueError('data_dir does not exist.')\n",
    "    if not os.path.exists(pkl_path):\n",
    "        raise ValueError('data_filename does not reside in data_dir.')\n",
    "    \n",
    "    dataset = Dataset(pkl_path)\n",
    "    train_raw_seqs, test_raw_seqs = dataset.get_splits(test_users)\n",
    "    train_triplets = [prepare_raw_seq(seq) for seq in train_raw_seqs]\n",
    "    train_input_seqs, train_reset_seqs, train_label_seqs = zip(*train_triplets)\n",
    "    test_triplets = [prepare_raw_seq(seq) for seq in test_raw_seqs]\n",
    "    test_input_seqs, test_reset_seqs, test_label_seqs = zip(*test_triplets)\n",
    "        \n",
    "    Model = eval(model_type + 'Model')\n",
    "    input_size = train_input_seqs[0].shape[1]\n",
    "    target_size = dataset.num_classes\n",
    "    \n",
    "    if gpu_ind is None or 'CUDA_VISIBLE_DEVICES' not in os.environ:\n",
    "        device = '/cpu:0'\n",
    "        config = None\n",
    "    else:\n",
    "        raw_devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = raw_devices[gpu_ind]\n",
    "        device = '/gpu:0'\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_mem_fraction)\n",
    "        config = tf.ConfigProto(gpu_options=gpu_options, intra_op_parallelism_threads=2,\n",
    "                                inter_op_parallelism_threads=2, allow_soft_placement=True)\n",
    "\n",
    "    with tf.device(device), tf.Graph().as_default(), tf.Session(config=config) as sess:\n",
    "        model = Model(input_size, target_size, params)\n",
    "        optimizer = Optimizer(model.loss, params)\n",
    "        train(sess, model, optimizer, train_input_seqs, train_reset_seqs, train_label_seqs,\n",
    "              test_input_seqs, test_reset_seqs, test_label_seqs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = Params()\n",
    "run_training(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
